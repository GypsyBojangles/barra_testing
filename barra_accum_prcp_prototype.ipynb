{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20190709/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/g/data/v10/public/modules/dea-env/20190709/lib/python3.6/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import uuid\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "from datacube import Datacube\n",
    "from datacube.index.hl import Doc2Dataset\n",
    "from datacube.utils import changes\n",
    "\n",
    "os.environ['GDAL_NETCDF_BOTTOMUP'] = 'NO'\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "def print_dict(doc):\n",
    "    print(json.dumps(doc, indent=4, sort_keys=True, cls=NumpySafeEncoder))\n",
    "\n",
    "def find_datasets(path: Path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    path = sorted(path.glob('**/*.nc'))\n",
    "    pattern = re.compile(r'(?P<barra_var>accum_prcp)\\-(?P<barra_var_type>fc\\-spec)\\-(?P<barra_sampling_frequency>PT1H)\\-(?P<barra_domain>BARRA_R)\\-(?P<barra_version>v1)\\-(?P<date>\\d{8})T(?P<time>\\d{4})Z\\.sub.nc')\n",
    "    datasets = defaultdict(dict)\n",
    "    for ncfile in path:\n",
    "        match = pattern.search(str(ncfile))\n",
    "        if match:\n",
    "            barra_var, barra_var_type, barra_sampling_frequency, barra_domain, barra_version, date, hour = match.groups()\n",
    "            dataset = barra_var + date + hour+ barra_domain + barra_version\n",
    "            datasets[dataset][barra_var] = ncfile\n",
    "    return datasets\n",
    "\n",
    "def generate_product_defn():\n",
    "    return {\n",
    "        'name': 'accum_prcp',\n",
    "        'metadata_type': 'eo',\n",
    "        'metadata': {\n",
    "            'product_type': 'barra_accum_prcp',\n",
    "            'format' : { 'name': 'NetCDF'}\n",
    "        },\n",
    "        'storage': {\n",
    "            'crs': 'GEOGCS[\"unknown\",DATUM[\"unknown\",SPHEROID[\"Sphere\",6371229,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]]',\n",
    "            'resolution': {\n",
    "                'latitude': 0.1100000035809413773,\n",
    "                'longitude': 0.1100000058540808734\n",
    "            },\n",
    "            'origin': { 'longitude':65, 'latitude':19.48}\n",
    "        },\n",
    "        'description': 'BARRA Hourly precipitation accumulation',\n",
    "        'measurements': [\n",
    "            {\n",
    "                'name':'accum_prcp',\n",
    "                'dtype':'float64',\n",
    "                'nodata': -1073741824,\n",
    "                'units':'kg m-2'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def generate_dataset_docs(dataset_name, dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sample_ncfile = dataset['accum_prcp']\n",
    "    sample_ncfile_gdal = f'NETCDF:{sample_ncfile}:accum_prcp'\n",
    "    creation_time = datetime.fromtimestamp(sample_ncfile.stat().st_mtime)\n",
    "    geo_ref_points, spatial_ref = get_grid_spatial_projection(\n",
    "        sample_ncfile_gdal)\n",
    "    \n",
    "    date = name[10:22]\n",
    "\n",
    "    start_time = datetime.strptime(date, '%Y%m%d%H%M')\n",
    "    end_time = start_time + timedelta(hours=1) - timedelta(microseconds=1)\n",
    "    center_time = start_time + timedelta(minutes=30)\n",
    "    docs = []\n",
    "    for i in range(6):\n",
    "        unique_ds_uri = f'{sample_ncfile.as_uri()}#{creation_time}#{start_time+timedelta(hours=i)}'\n",
    "        doc = {\n",
    "            'id': str(uuid.uuid5(uuid.NAMESPACE_URL, unique_ds_uri)),\n",
    "            'product_type': 'barra_accum_prcp',\n",
    "            'creation_dt': str(creation_time),\n",
    "            'extent': {\n",
    "                'from_dt': str(start_time+timedelta(hours=i)),\n",
    "                'to_dt': str(end_time+timedelta(hours=i)),\n",
    "                'center_dt': str(center_time+timedelta(hours=i)),\n",
    "                'coord': to_lat_long_extent(geo_ref_points,spatial_ref),\n",
    "            },\n",
    "            'format': {'name': 'NetCDF'},\n",
    "            'grid_spatial': {\n",
    "                'projection': {\n",
    "                    'geo_ref_points': geo_ref_points,\n",
    "                    'spatial_reference': spatial_ref,\n",
    "                }\n",
    "            },\n",
    "            'image': {\n",
    "                'bands': {\n",
    "                    'accum_prcp': {\n",
    "                        'path': '',\n",
    "                        'layer': 'accum_prcp',\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'lineage': {'source_datasets': {}}\n",
    "        }\n",
    "        docs.append(('file:'+str(dataset['accum_prcp'])+'#part='+str(i),doc))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def to_lat_long_extent(geo_ref_points, spatial_ref):\n",
    "    source = osr.SpatialReference(spatial_ref)\n",
    "\n",
    "    target = osr.SpatialReference()\n",
    "    target.ImportFromEPSG(4326)\n",
    "\n",
    "    transform = osr.CoordinateTransformation(source, target)\n",
    "    \n",
    "    lat_long_extent = {}\n",
    "    for corner, points in geo_ref_points.items():\n",
    "        \n",
    "        point = ogr.CreateGeometryFromWkt(\"POINT (\"+str(points['y'])+\" \"+str(points['x'])+\")\")\n",
    "        point.Transform(transform)\n",
    "        \n",
    "        lat_long_extent[corner] = {'lat': str(point.GetY()), 'lon': str(point.GetX())}\n",
    "    \n",
    "    return lat_long_extent\n",
    "\n",
    "\n",
    "def get_grid_spatial_projection(fname):\n",
    "    with rasterio.open(fname, 'r') as img:\n",
    "        left, bottom, right, top = img.bounds\n",
    "        spatial_reference = str(str(getattr(img, 'crs_wkt', None) or img.crs.wkt))\n",
    "        geo_ref_points = {\n",
    "            'ul': {'x': left, 'y': top},\n",
    "            'ur': {'x': right, 'y': top},\n",
    "            'll': {'x': left, 'y': bottom},\n",
    "            'lr': {'x': right, 'y': bottom},\n",
    "        }\n",
    "        return geo_ref_points, spatial_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetType(name='accum_prcp', id_=12)\n"
     ]
    }
   ],
   "source": [
    "dc = Datacube(config='datacube.conf',env='barra-dev')\n",
    "index = dc.index\n",
    "\n",
    "product_def = generate_product_defn()\n",
    "product = index.products.from_doc(product_def)\n",
    "indexed_product = index.products.add(product)\n",
    "print(indexed_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = Path('/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/')\n",
    "datasets = find_datasets(path)\n",
    "resolver = Doc2Dataset(index)\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    docs = generate_dataset_docs(name, dataset)\n",
    "    for doc in docs:\n",
    "        dataset, err = resolver(doc[1], doc[0])\n",
    "        try:\n",
    "            indexed_dataset = index.datasets.add(dataset)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Couldn't index %s/%s\", path, name)\n",
    "            logging.exception(\"Exception\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
